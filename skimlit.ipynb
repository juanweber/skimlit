{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5606826-7595-415f-a6c1-29163a003668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Juan\\\\skimlit'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc238e9c-0cd5-48c8-b95f-d160f51a8ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\Jupyter projects\\\\skimlit') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c04c33c-671e-4718-ab63-4584da8708dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Jupyter projects\\\\skimlit'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d63c7b-f66e-46d4-89e7-d5f2d30baccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3060 (UUID: GPU-e8861a5c-89dd-5ee2-06b2-d3bca4ee7bc6)\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39cddd9c-3468-4524-91e8-00d76eacc12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'pubmed-rct'...\n",
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82fbcf78-9aa2-4133-b4a0-571ba51e7bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is DATA\n",
      " Volume Serial Number is B29C-03D0\n",
      "\n",
      " Directory of D:\\Jupyter projects\\skimlit\\pubmed-rct\n",
      "\n",
      "31/10/2022  22:23    <DIR>          .\n",
      "31/10/2022  22:23    <DIR>          ..\n",
      "31/10/2022  22:23    <DIR>          PubMed_200k_RCT\n",
      "31/10/2022  22:23    <DIR>          PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
      "31/10/2022  22:23    <DIR>          PubMed_20k_RCT\n",
      "31/10/2022  22:23    <DIR>          PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
      "31/10/2022  22:23             2,384 README.md\n",
      "               1 File(s)          2,384 bytes\n",
      "               6 Dir(s)  409,996,783,616 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir pubmed-rct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017c28a4-9914-451a-a40a-20ca8fedbc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter format not correct - \"PubMed_20k_RCT_numbers_replaced_with_at_sign\".\n"
     ]
    }
   ],
   "source": [
    "# Check what files are in the PubMed_20K dataset \n",
    "!dir pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837f29da-e8fe-43ba-8b89-fbe39efb57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by using the 20k dataset\n",
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c41cfd-08bb-4db8-8e19-7951332e5579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all of the filenames in the target directory\n",
    "import os\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20070473-605d-42f5-b05b-3d6ceb1d21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "    \"\"\"\n",
    "    Reads filename (a text file) and returns the lines of text as a list.\n",
    "    \n",
    "    Args:\n",
    "        filename: a string containing the target filepath to read.\n",
    "    \n",
    "    Returns:\n",
    "        A list of strings with one string per line from the target filename.\n",
    "        For example:\n",
    "        [\"this is the first line of filename\",\n",
    "         \"this is the second line of filename\",\n",
    "         \"...\"]\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175d19c0-bb68-4803-9db5-a71653441161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n',\n",
       " 'METHODS\\tSecondary outcome measures included the Western Ontario and McMaster Universities Osteoarthritis Index scores , patient global assessment ( PGA ) of the severity of knee OA , and @-min walk distance ( @MWD ) .\\n',\n",
       " 'METHODS\\tSerum levels of interleukin @ ( IL-@ ) , IL-@ , tumor necrosis factor ( TNF ) - , and high-sensitivity C-reactive protein ( hsCRP ) were measured .\\n',\n",
       " 'RESULTS\\tThere was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , PGA , and @MWD at @ weeks .\\n',\n",
       " 'RESULTS\\tThe mean difference between treatment arms ( @ % CI ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .\\n',\n",
       " 'RESULTS\\tFurther , there was a clinically relevant reduction in the serum levels of IL-@ , IL-@ , TNF - , and hsCRP at @ weeks in the intervention group when compared to the placebo group .\\n',\n",
       " 'RESULTS\\tThese differences remained significant at @ weeks .\\n',\n",
       " 'RESULTS\\tThe Outcome Measures in Rheumatology Clinical Trials-Osteoarthritis Research Society International responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .\\n',\n",
       " 'CONCLUSIONS\\tLow-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee OA ( ClinicalTrials.gov identifier NCT@ ) .\\n',\n",
       " '\\n',\n",
       " '###24854809\\n',\n",
       " 'BACKGROUND\\tEmotional eating is associated with overeating and the development of obesity .\\n',\n",
       " 'BACKGROUND\\tYet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .\\n',\n",
       " 'OBJECTIVE\\tThe aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .\\n',\n",
       " 'OBJECTIVE\\tIt was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .\\n',\n",
       " 'METHODS\\tParticipants ( N = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines = get_lines(data_dir+\"train.txt\")\n",
    "train_lines[:20] # the whole first example of an abstract + a little more of the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18dcb396-cf8c-4a3c-ad27-4cdc2d18a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\"Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "    Takes in filename, reads its contents and sorts through each line,\n",
    "    extracting things like the target label, the text of the sentence,\n",
    "    how many sentences are in the current abstract and what sentence number\n",
    "    the target line is.\n",
    "\n",
    "    Args:\n",
    "        filename: a string of the target text file to read and extract line data\n",
    "        from.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries each containing a line from an abstract,\n",
    "        the lines label, the lines position in the abstract and the total number\n",
    "        of lines in the abstract where the line is from. For example:\n",
    "\n",
    "        [{\"target\": 'CONCLUSION',\n",
    "          \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
    "          \"line_number\": 8,\n",
    "          \"total_lines\": 8}]\n",
    "    \"\"\"\n",
    "    input_lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "    \n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" #reset abstract string\n",
    "        elif line.isspace():\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "            \n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_data = {}\n",
    "                target_text_split = abstract_line.split(\"\\t\")#split target label from text\n",
    "                line_data[\"target\"] = target_text_split[0] #get target label\n",
    "                line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "                line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "                line_data[\"total_lines\"] = len(abstract_line_split) - 1 ## how many total lines are in the abstract? (start from 0)\n",
    "                abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "                \n",
    "        else: #if above conditions arent fulfilled, the line contains the labelled sentence\n",
    "            abstract_lines += line\n",
    "            \n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da485153-7b5a-4f95-8ca7-0ff26aeaf7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 647 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") #dev=validation set\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n",
    "\n",
    "len(train_samples), len(val_samples), len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5b5e5a-8646-4d8e-a5a3-7729ca7dabac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'these differences remained significant at @ weeks .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 11},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 11},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'emotional eating is associated with overeating and the development of obesity .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 10},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 10}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check first abstract of training data\n",
    "train_samples[:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b90a7e1e-48e3-481a-b1d4-07e54a8b909a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>serum levels of interleukin @ ( il-@ ) , il-@ ...</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>there was a clinically relevant reduction in t...</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the mean difference between treatment arms ( @...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>further , there was a clinically relevant redu...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>these differences remained significant at @ we...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>the outcome measures in rheumatology clinical ...</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>low-dose oral prednisolone had both a short-te...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>emotional eating is associated with overeating...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>yet , empirical evidence for individual ( trai...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text  \\\n",
       "0     OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
       "1       METHODS  a total of @ patients with primary knee oa wer...   \n",
       "2       METHODS  outcome measures included pain reduction and i...   \n",
       "3       METHODS  pain was assessed using the visual analog pain...   \n",
       "4       METHODS  secondary outcome measures included the wester...   \n",
       "5       METHODS  serum levels of interleukin @ ( il-@ ) , il-@ ...   \n",
       "6       RESULTS  there was a clinically relevant reduction in t...   \n",
       "7       RESULTS  the mean difference between treatment arms ( @...   \n",
       "8       RESULTS  further , there was a clinically relevant redu...   \n",
       "9       RESULTS  these differences remained significant at @ we...   \n",
       "10      RESULTS  the outcome measures in rheumatology clinical ...   \n",
       "11  CONCLUSIONS  low-dose oral prednisolone had both a short-te...   \n",
       "12   BACKGROUND  emotional eating is associated with overeating...   \n",
       "13   BACKGROUND  yet , empirical evidence for individual ( trai...   \n",
       "\n",
       "    line_number  total_lines  \n",
       "0             0           11  \n",
       "1             1           11  \n",
       "2             2           11  \n",
       "3             3           11  \n",
       "4             4           11  \n",
       "5             5           11  \n",
       "6             6           11  \n",
       "7             7           11  \n",
       "8             8           11  \n",
       "9             9           11  \n",
       "10           10           11  \n",
       "11           11           11  \n",
       "12            0           10  \n",
       "13            1           10  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "train_df.head(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e49f6d9-d02e-4bf2-a4a6-5da5f27d5e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b265376-c8ee-45ca-ac1d-2d23fd2db932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1dd62f28670>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.total_lines.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12142699-55ed-4a54-af55-1284ca5c667d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert abstract text lines into lists \n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "\n",
    "len(train_sentences), len(val_sentences), len(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72ad7dd8-ac2a-429b-9e27-e8397f0b0261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       " 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       " 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       " 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       " 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       " 'these differences remained significant at @ weeks .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be77ccc4-9314-48bd-ad18-330536928bf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encode labels\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.fit_transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "#check training labels\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc5272c-f196-4f0b-9899-5247e18568cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Label encode labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.fit_transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.fit_transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "#check training labels\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "071e6b3b-a660-41ee-8673-584778d60d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,\n",
       " array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get class names and number of classes from LabelEncoder instance\n",
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "num_classes, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11c02238-dc98-41c6-91d6-ce075e98b46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tf-idf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#create pipeline\n",
    "model_0 = Pipeline([\n",
    "    (\"tf-idf\", TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "#fit pipeline to training data\n",
    "model_0.fit(X=train_sentences,\n",
    "            y=train_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67c52d48-0256-4829-888c-700d97995196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218323844829869"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate baseline on validation set\n",
    "model_0.score(X=val_sentences,\n",
    "              y=val_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e52901ed-373f-46ab-ab10-1fac18bd9ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, ..., 4, 4, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "632214c5-d3fc-421f-8e60-37cd06fa858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-11-02 16:53:19--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10246 (10K) [text/plain]\n",
      "Saving to: 'helper_functions.py'\n",
      "\n",
      "     0K ..........                                            100% 3.28M=0.003s\n",
      "\n",
      "2022-11-02 16:53:20 (3.28 MB/s) - 'helper_functions.py' saved [10246/10246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download helper functions script\n",
    "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9a57207-b524-4f17-9bb9-4af0b4e3465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import calculate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cb25b03-f9df-47a3-91d7-badaf74c77a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 72.1832384482987,\n",
       " 'precision': 0.7186466952323352,\n",
       " 'recall': 0.7218323844829869,\n",
       " 'f1': 0.6989250353450294}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                     y_pred=baseline_preds)\n",
    "\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d0fc0c7-b24f-421a-ac2c-70770f471f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edad40ac-5bd6-4ba8-a979-c505f6f6e751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how long is each sentence on average?\n",
    "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_sent_len = np.mean(sent_lens)\n",
    "avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a71068f5-9990-42f2-94c0-e660e7af006a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#distribution\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(sent_lens, bins=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e08292f5-697a-40fb-8a76-4d7b3de0552c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how long of a sentence covers 95% of the lengths?\n",
    "output_seq_len = int(np.percentile(sent_lens, 95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b786a6c5-48a7-4312-8e76-e23db701cad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many words are in our vocabulary? (taken from 3.2 in https://arxiv.org/pdf/1710.06071.pdf)\n",
    "max_tokens = 68000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ddbce-4767-4850-b99e-749e06a8a3d0",
   "metadata": {},
   "source": [
    "And since discovered a sentence length of 55 covers 95% of the training sentences, we'll use that as our output_sequence_length parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4866715-4dd7-48ec-a2ed-5b8a3bd2d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create text vectorizer\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens, #number of words in vocab\n",
    "                                    output_sequence_length=55) #desired output length of vectorized sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3f6cbef-4f43-41d8-98e2-d41bd6c7b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapt text vectorizer to training sentences\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01193473-07af-4600-80f2-5b80d35bbfb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "no deaths occurred .\n",
      "\n",
      "Length of text: 4\n",
      "\n",
      "Vectorized text:\n",
      "[[  33 1254  344    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "#test text vectorizer\n",
    "import random\n",
    "target_sentence = random.choice(train_sentences)\n",
    "print(f\"Text:\\n{target_sentence}\")\n",
    "print(f\"\\nLength of text: {len(target_sentence.split())}\")\n",
    "print(f\"\\nVectorized text:\\n{text_vectorizer([target_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fe729320-aeef-4416-a749-188e0e33cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 64841\n",
      "Most common words in the vocabulary: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Least common words in the vocabulary: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "#how many words in our training vocab?\n",
    "rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Number of words in vocabulary: {len(rct_20k_text_vocab)}\"), \n",
    "print(f\"Most common words in the vocabulary: {rct_20k_text_vocab[:5]}\")\n",
    "print(f\"Least common words in the vocabulary: {rct_20k_text_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "975efbef-25c4-47b9-bdb2-68b5372aba87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'text_vectorization',\n",
       " 'trainable': True,\n",
       " 'batch_input_shape': (None,),\n",
       " 'dtype': 'string',\n",
       " 'max_tokens': 68000,\n",
       " 'standardize': 'lower_and_strip_punctuation',\n",
       " 'split': 'whitespace',\n",
       " 'ngrams': None,\n",
       " 'output_mode': 'int',\n",
       " 'output_sequence_length': 55,\n",
       " 'pad_to_max_tokens': False,\n",
       " 'sparse': False,\n",
       " 'ragged': False,\n",
       " 'vocabulary': None,\n",
       " 'idf_weights': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the config of our text vectorizer\n",
    "text_vectorizer.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87347cfd-811a-4710-afa4-a79c7e9db886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence before vectorization:\n",
      "no deaths occurred .\n",
      "\n",
      "Sentence after vectorization (before embedding):\n",
      "[[  33 1254  344    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Sentence after embedding:\n",
      "[[[ 0.04249867 -0.02054707  0.02040435 ... -0.03401254  0.03974767\n",
      "    0.00280558]\n",
      "  [ 0.02417399 -0.04129678 -0.04778774 ...  0.04807645 -0.04934117\n",
      "    0.01046914]\n",
      "  [ 0.00529259 -0.04349089  0.04941614 ... -0.00886861  0.0481861\n",
      "   -0.01885399]\n",
      "  ...\n",
      "  [ 0.04702115 -0.02689379 -0.0290525  ...  0.04582962  0.03493682\n",
      "   -0.00019791]\n",
      "  [ 0.04702115 -0.02689379 -0.0290525  ...  0.04582962  0.03493682\n",
      "   -0.00019791]\n",
      "  [ 0.04702115 -0.02689379 -0.0290525  ...  0.04582962  0.03493682\n",
      "   -0.00019791]]]\n",
      "\n",
      "Embedded sentence shape: (1, 55, 128)\n"
     ]
    }
   ],
   "source": [
    "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab), # length of vocabulary\n",
    "                               output_dim=128,# Note: different embedding sizes result in drastically different numbers of parameters to train\n",
    "                               # Use masking to handle variable sequence lengths (save space)\n",
    "                               mask_zero=True,\n",
    "                               name=\"token_embedding\"\n",
    "                              )\n",
    "\n",
    "# Show example embedding\n",
    "print(f\"Sentence before vectorization:\\n{target_sentence}\\n\")\n",
    "vectorized_sentence = text_vectorizer([target_sentence])\n",
    "print(f\"Sentence after vectorization (before embedding):\\n{vectorized_sentence}\\n\")\n",
    "embedded_sentence = token_embed(vectorized_sentence)\n",
    "print(f\"Sentence after embedding:\\n{embedded_sentence}\\n\")\n",
    "print(f\"Embedded sentence shape: {embedded_sentence.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75c2ba67-6bca-48f7-b4de-31e324ac4c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "990d5bc0-259b-4d9d-8897-a26c94f8aebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take TensorSliceDataset and turn them into prefetched batches\n",
    "\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37ce5068-1d82-4249-b67c-5a9025b04b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1: Conv1D with token embeddings\n",
    "# Create 1D convolutional model to process sequences\n",
    "\n",
    "inputs = layers.Input(shape=(1, ), dtype=tf.string)\n",
    "text_vectors = text_vectorizer(inputs) # vectorize text inputs\n",
    "token_embeddings = token_embed(text_vectors) # create embeddings\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation='relu')(token_embeddings)\n",
    "x = layers.GlobalAveragePooling1D()(x) #condense output of feature vector\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model_1.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7944517-eb55-46a8-a80e-474aea18f91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " token_embedding (Embedding)  (None, 55, 128)          8299648   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b484833-b2bd-4cc2-a1d3-dc07db081b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5627"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ff84b12-28ce-41c4-b406-541a47d2909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 13s 8ms/step - loss: 0.9203 - accuracy: 0.6393 - val_loss: 0.6842 - val_accuracy: 0.7410\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 4s 8ms/step - loss: 0.6551 - accuracy: 0.7570 - val_loss: 0.6342 - val_accuracy: 0.7736\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 4s 8ms/step - loss: 0.6139 - accuracy: 0.7759 - val_loss: 0.5948 - val_accuracy: 0.7869\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_dataset)), # only fit on 10% of batches for faster training time\n",
    "                              epochs=3,\n",
    "                              validation_data=valid_dataset,\n",
    "                              validation_steps=int(0.1 * len(valid_dataset))) # only validate on 10% of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3429dc48-81e4-4a41-b2f9-84d20914c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 4s 4ms/step - loss: 0.5949 - accuracy: 0.7880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5949373245239258, 0.7879650592803955]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on whole validation dataset (we only validated on 10% of batches during training)\n",
    "model_1.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de0ed93a-cd45-4fb8-a47c-74578b9203ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.6994212e-01, 1.4422594e-01, 5.3681195e-02, 3.0807778e-01,\n",
       "        2.4072979e-02],\n",
       "       [4.1728234e-01, 3.0866230e-01, 1.2770690e-02, 2.5216243e-01,\n",
       "        9.1222413e-03],\n",
       "       [1.5057959e-01, 5.4801204e-03, 2.1188292e-03, 8.4177375e-01,\n",
       "        4.7712401e-05],\n",
       "       ...,\n",
       "       [1.0408709e-05, 6.5695727e-04, 1.0600114e-03, 4.6237683e-06,\n",
       "        9.9826801e-01],\n",
       "       [5.3442825e-02, 4.5694879e-01, 7.5917602e-02, 6.2281877e-02,\n",
       "        3.5140887e-01],\n",
       "       [1.7071326e-01, 6.9483531e-01, 4.6577584e-02, 5.5633642e-02,\n",
       "        3.2240123e-02]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4451682c-9f27-4af9-aaf0-b0ebb092ec8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f83d811-9c5d-41c5-8cf5-f6b643d77e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_1_preds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "055ff7cc-4f4b-4ff7-8663-6a769ace7a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 78.79650470011916,\n",
       " 'precision': 0.78526292366117,\n",
       " 'recall': 0.7879650470011916,\n",
       " 'f1': 0.7857385360883143}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abb9898b-b87a-4bed-9280-4dc7dd4d4445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: Feature extraction with pretrained token embeddings\n",
    "import tensorflow_hub as hub\n",
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False,\n",
    "                                        name='universal_sentence_encoder'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e99e75ad-14e9-4337-927d-9818de501682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random training sentence:\n",
      "to compare the effects of neuromuscular exercise ( nexa ) and quadriceps strengthening ( qs ) on the knee adduction moment ( an indicator of mediolateral distribution of knee load ) , pain , and physical function in patients with medial knee joint osteoarthritis ( oa ) and varus malalignment .\n",
      "\n",
      "Sentence after embedding:\n",
      "[ 0.06259711 -0.03440529  0.05215479 -0.03296117 -0.00900072  0.03169783\n",
      "  0.02502066 -0.02815966 -0.03417582  0.06331593  0.06629574  0.01335277\n",
      " -0.05694374  0.04772395  0.04914102 -0.01046281 -0.07444877 -0.03298623\n",
      "  0.01406145 -0.05365983 -0.04767127  0.01737566  0.04353544  0.01316874\n",
      "  0.03506197 -0.01513516  0.05539828 -0.00713142 -0.0028914  -0.06388082] (truncated output)...\n",
      "\n",
      "Length of sentence embedding:\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# Test out the embedding on a random sentence\n",
    "random_training_sentence = random.choice(train_sentences)\n",
    "print(f\"Random training sentence:\\n{random_training_sentence}\\n\")\n",
    "use_embedded_sentence = tf_hub_embedding_layer([random_training_sentence])\n",
    "print(f\"Sentence after embedding:\\n{use_embedded_sentence[0][:30]} (truncated output)...\\n\")\n",
    "print(f\"Length of sentence embedding:\\n{len(use_embedded_sentence[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f1cd895-3832-498b-ad0a-68e295161ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embedding = tf_hub_embedding_layer(inputs) #tokenize text and create embedding\n",
    "x = layers.Dense(128, activation='relu')(pretrained_embedding)\n",
    "outputs = layers.Dense(5, activation='softmax')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "                optimizer='Adam',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8184580f-c1af-4028-a0e7-ed08f6641759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None,)]                 0         \n",
      "                                                                 \n",
      " universal_sentence_encoder   (None, 512)              256797824 \n",
      " (KerasLayer)                                                    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 256,864,133\n",
      "Trainable params: 66,309\n",
      "Non-trainable params: 256,797,824\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "82f525b4-96bf-4b43-ba7d-11b1b1087b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 12s 18ms/step - loss: 0.9202 - accuracy: 0.6453 - val_loss: 0.7946 - val_accuracy: 0.6902\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 8s 14ms/step - loss: 0.7686 - accuracy: 0.7003 - val_loss: 0.7546 - val_accuracy: 0.7038\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 9s 16ms/step - loss: 0.7516 - accuracy: 0.7126 - val_loss: 0.7381 - val_accuracy: 0.7164\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e007c831c0>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(train_dataset,\n",
    "           steps_per_epoch = int(0.1 * len(train_dataset)),\n",
    "            epochs=3,\n",
    "            validation_data=valid_dataset,\n",
    "            validation_steps=int(0.1 * len(valid_dataset))\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32ca2cc4-a187-4e76-afce-53c643c32fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 13s 14ms/step - loss: 0.7417 - accuracy: 0.7145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.741651177406311, 0.7145174145698547]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a591c05-399f-49f5-8e07-6706c5b90c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 71.45174103005428,\n",
       " 'precision': 0.7151589040740375,\n",
       " 'recall': 0.7145174103005428,\n",
       " 'f1': 0.7115372830564406}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2_pred_probs = model_2.predict(valid_dataset)\n",
    "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
    "# Calculate results from TF Hub pretrained embeddings results on validation set\n",
    "model_2_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a91080e6-20f7-44f3-bfbc-770bc44ae5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t o   c o m p a r e   t h e   e f f e c t s   o f   n e u r o m u s c u l a r   e x e r c i s e   (   n e x a   )   a n d   q u a d r i c e p s   s t r e n g t h e n i n g   (   q s   )   o n   t h e   k n e e   a d d u c t i o n   m o m e n t   (   a n   i n d i c a t o r   o f   m e d i o l a t e r a l   d i s t r i b u t i o n   o f   k n e e   l o a d   )   ,   p a i n   ,   a n d   p h y s i c a l   f u n c t i o n   i n   p a t i e n t s   w i t h   m e d i a l   k n e e   j o i n t   o s t e o a r t h r i t i s   (   o a   )   a n d   v a r u s   m a l a l i g n m e n t   .'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model 3: Conv1D with character embeddings\n",
    "\n",
    "# Make function to split sentences into characters\n",
    "def split_chars(text):\n",
    "    return \" \".join(list(text))\n",
    "\n",
    "split_chars(random_training_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3390678a-9ecb-4846-a7d7-704df23ec0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .\n"
     ]
    }
   ],
   "source": [
    "#split sequence level data splits into character level data splits\n",
    "\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "\n",
    "print(train_chars[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9f04efe-b4c3-4324-8770-d5e01fcfd5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the average character length?\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "126782ee-8455-4f7c-a99e-d309c832ae47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.41175e+05, 3.71110e+04, 1.60000e+03, 1.27000e+02, 2.10000e+01,\n",
       "        5.00000e+00, 1.00000e+00]),\n",
       " array([1.00000000e+00, 1.98857143e+02, 3.96714286e+02, 5.94571429e+02,\n",
       "        7.92428571e+02, 9.90285714e+02, 1.18814286e+03, 1.38600000e+03]),\n",
       " <a list of 7 Patch objects>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD4CAYAAAAdIcpQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAW5klEQVR4nO3df6zd9X3f8edrdkMgGcSAodS2dp1idQO0LcEipJ2qaO7ATSLMHyA5aoa3erKG2JZ2q1I8pLIlsgRrVVrUwYQCxdAMsNx0WIlYYkGraBIxufnJr1BuCoUbHHw7U8paQWL63h/nc5Xjm+P7se/1/UF5PqSj8z3v7/fzve/v1fV93e/38z3HqSokSZrN31vqBiRJy59hIUnqMiwkSV2GhSSpy7CQJHWtXOoGTrazzz67xsbGlroNSXpL+drXvvYXVbX6WOv/zoXF2NgY4+PjS92GJL2lJPnz2dZ7GUqS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHV1wyLJXUkOJXlixLpfS1JJzh6q7UwykeSZJJcP1S9O8nhbd2uStPopSR5o9QNJxobGbEvybHtsm+/BSpLm5njOLO4GNs8sJlkH/AvghaHaBcBW4MI25rYkK9rq24EdwIb2mN7nduCVqjofuAW4ue3rTOBG4APAJcCNSVad2OFJkk6G7ju4q+rLw3/tD7kF+CTw4FBtC3B/Vb0BPJdkArgkyfPA6VX1KECSe4ArgYfamP/Sxu8Ffq+ddVwO7K+qw23MfgYBc9+JHeKJGbv+Cwu5+5Pq+Zs+stQtSHqbmNOcRZIrgO9V1bdmrFoDvDj0erLV1rTlmfWjxlTVEeBV4KxZ9jWqnx1JxpOMT01NzeWQJEmzOOGwSHIacAPwG6NWj6jVLPW5jjm6WHVHVW2sqo2rVx/zc7AkSXM0lzOLnwbWA99ql5fWAl9P8pMM/vpfN7TtWuClVl87os7wmCQrgTOAw7PsS5K0yE44LKrq8ao6p6rGqmqMwS/191fV94F9wNZ2h9N6BhPZj1XVQeC1JJe2+Yhr+NFcxz5g+k6nq4BHqqqALwKXJVnVJrYvazVJ0iLrTnAnuQ/4EHB2kkngxqq6c9S2VfVkkj3AU8AR4LqqerOtvpbBnVWnMpjYfqjV7wTubZPhhxncTUVVHU7yaeCrbbtPTU92S5IW1/HcDfWxzvqxGa93AbtGbDcOXDSi/jpw9TH2fRdwV69HSdLC8h3ckqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSerqhkWSu5IcSvLEUO03k3wnybeT/FGS9wyt25lkIskzSS4fql+c5PG27tYkafVTkjzQ6geSjA2N2Zbk2fbYdrIOWpJ0Yo7nzOJuYPOM2n7goqr6x8CfAjsBklwAbAUubGNuS7Kijbkd2AFsaI/pfW4HXqmq84FbgJvbvs4EbgQ+AFwC3Jhk1YkfoiRpvrphUVVfBg7PqH2pqo60l18B1rblLcD9VfVGVT0HTACXJDkPOL2qHq2qAu4Brhwas7st7wU2tbOOy4H9VXW4ql5hEFAzQ0uStAhOxpzFLwMPteU1wItD6yZbbU1bnlk/akwLoFeBs2bZ149JsiPJeJLxqampeR2MJOnHzSssktwAHAE+O10asVnNUp/rmKOLVXdU1caq2rh69erZm5YknbA5h0WbcP4o8Evt0hIM/vpfN7TZWuClVl87on7UmCQrgTMYXPY61r4kSYtsTmGRZDPw68AVVfU3Q6v2AVvbHU7rGUxkP1ZVB4HXklza5iOuAR4cGjN9p9NVwCMtfL4IXJZkVZvYvqzVJEmLbGVvgyT3AR8Czk4yyeAOpZ3AKcD+dgfsV6rq31bVk0n2AE8xuDx1XVW92XZ1LYM7q05lMMcxPc9xJ3BvkgkGZxRbAarqcJJPA19t232qqo6aaJckLY5uWFTVx0aU75xl+13ArhH1ceCiEfXXgauPsa+7gLt6PUqSFpbv4JYkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkrq6YZHkriSHkjwxVDszyf4kz7bnVUPrdiaZSPJMksuH6hcnebytuzVJWv2UJA+0+oEkY0NjtrWv8WySbSfroCVJJ+Z4zizuBjbPqF0PPFxVG4CH22uSXABsBS5sY25LsqKNuR3YAWxoj+l9bgdeqarzgVuAm9u+zgRuBD4AXALcOBxKkqTF0w2LqvoycHhGeQuwuy3vBq4cqt9fVW9U1XPABHBJkvOA06vq0aoq4J4ZY6b3tRfY1M46Lgf2V9XhqnoF2M+Ph5YkaRHMdc7i3Ko6CNCez2n1NcCLQ9tNttqatjyzftSYqjoCvAqcNcu+fkySHUnGk4xPTU3N8ZAkScdysie4M6JWs9TnOuboYtUdVbWxqjauXr36uBqVJB2/uYbFy+3SEu35UKtPAuuGtlsLvNTqa0fUjxqTZCVwBoPLXsfalyRpkc01LPYB03cnbQMeHKpvbXc4rWcwkf1Yu1T1WpJL23zENTPGTO/rKuCRNq/xReCyJKvaxPZlrSZJWmQrexskuQ/4EHB2kkkGdyjdBOxJsh14AbgaoKqeTLIHeAo4AlxXVW+2XV3L4M6qU4GH2gPgTuDeJBMMzii2tn0dTvJp4Kttu09V1cyJdknSIuiGRVV97BirNh1j+13ArhH1ceCiEfXXaWEzYt1dwF29HiVJC8t3cEuSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV3zCoskv5rkySRPJLkvyTuTnJlkf5Jn2/Oqoe13JplI8kySy4fqFyd5vK27NUla/ZQkD7T6gSRj8+lXkjQ3cw6LJGuA/wBsrKqLgBXAVuB64OGq2gA83F6T5IK2/kJgM3BbkhVtd7cDO4AN7bG51bcDr1TV+cAtwM1z7VeSNHfzvQy1Ejg1yUrgNOAlYAuwu63fDVzZlrcA91fVG1X1HDABXJLkPOD0qnq0qgq4Z8aY6X3tBTZNn3VIkhbPnMOiqr4H/BbwAnAQeLWqvgScW1UH2zYHgXPakDXAi0O7mGy1NW15Zv2oMVV1BHgVOGtmL0l2JBlPMj41NTXXQ5IkHcN8LkOtYvCX/3rgp4B3Jfn4bENG1GqW+mxjji5U3VFVG6tq4+rVq2dvXJJ0wuZzGeoXgOeqaqqqfgh8DvhZ4OV2aYn2fKhtPwmsGxq/lsFlq8m2PLN+1Jh2qesM4PA8epYkzcF8wuIF4NIkp7V5hE3A08A+YFvbZhvwYFveB2xtdzitZzCR/Vi7VPVakkvbfq6ZMWZ6X1cBj7R5DUnSIlo514FVdSDJXuDrwBHgG8AdwLuBPUm2MwiUq9v2TybZAzzVtr+uqt5su7sWuBs4FXioPQDuBO5NMsHgjGLrXPuVJM3dnMMCoKpuBG6cUX6DwVnGqO13AbtG1MeBi0bUX6eFjSRp6fgObklSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1zSsskrwnyd4k30nydJIPJjkzyf4kz7bnVUPb70wykeSZJJcP1S9O8nhbd2uStPopSR5o9QNJxubTryRpbuZ7ZvG7wP+uqn8I/BPgaeB64OGq2gA83F6T5AJgK3AhsBm4LcmKtp/bgR3AhvbY3OrbgVeq6nzgFuDmefYrSZqDOYdFktOBnwfuBKiqH1TVXwJbgN1ts93AlW15C3B/Vb1RVc8BE8AlSc4DTq+qR6uqgHtmjJne115g0/RZhyRp8cznzOK9wBTw+0m+keQzSd4FnFtVBwHa8zlt+zXAi0PjJ1ttTVueWT9qTFUdAV4FzprZSJIdScaTjE9NTc3jkCRJo8wnLFYC7wdur6r3AX9Nu+R0DKPOCGqW+mxjji5U3VFVG6tq4+rVq2fvWpJ0wuYTFpPAZFUdaK/3MgiPl9ulJdrzoaHt1w2NXwu81OprR9SPGpNkJXAGcHgePUuS5mDOYVFV3wdeTPIzrbQJeArYB2xrtW3Ag215H7C13eG0nsFE9mPtUtVrSS5t8xHXzBgzva+rgEfavIYkaRGtnOf4fw98Nsk7gD8D/jWDANqTZDvwAnA1QFU9mWQPg0A5AlxXVW+2/VwL3A2cCjzUHjCYPL83yQSDM4qt8+xXkjQH8wqLqvomsHHEqk3H2H4XsGtEfRy4aET9dVrYSJKWju/gliR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSuuYdFklWJPlGks+312cm2Z/k2fa8amjbnUkmkjyT5PKh+sVJHm/rbk2SVj8lyQOtfiDJ2Hz7lSSduJNxZvEJ4Omh19cDD1fVBuDh9pokFwBbgQuBzcBtSVa0MbcDO4AN7bG51bcDr1TV+cAtwM0noV9J0gmaV1gkWQt8BPjMUHkLsLst7wauHKrfX1VvVNVzwARwSZLzgNOr6tGqKuCeGWOm97UX2DR91iFJWjzzPbP4HeCTwN8O1c6tqoMA7fmcVl8DvDi03WSrrWnLM+tHjamqI8CrwFkzm0iyI8l4kvGpqal5HpIkaaY5h0WSjwKHquprxztkRK1mqc825uhC1R1VtbGqNq5evfo425EkHa+V8xj7c8AVST4MvBM4PckfAC8nOa+qDrZLTIfa9pPAuqHxa4GXWn3tiPrwmMkkK4EzgMPz6FmSNAdzPrOoqp1VtbaqxhhMXD9SVR8H9gHb2mbbgAfb8j5ga7vDaT2DiezH2qWq15Jc2uYjrpkxZnpfV7Wv8WNnFpKkhTWfM4tjuQnYk2Q78AJwNUBVPZlkD/AUcAS4rqrebGOuBe4GTgUeag+AO4F7k0wwOKPYugD9vmWNXf+FpW7hhDx/00eWugVJc3RSwqKq/gT4k7b8f4FNx9huF7BrRH0cuGhE/XVa2EiSlo7v4JYkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkrrmHBZJ1iX54yRPJ3kyySda/cwk+5M8255XDY3ZmWQiyTNJLh+qX5zk8bbu1iRp9VOSPNDqB5KMzf1QJUlzNZ8ziyPAf6qqfwRcClyX5ALgeuDhqtoAPNxe09ZtBS4ENgO3JVnR9nU7sAPY0B6bW3078EpVnQ/cAtw8j34lSXM057CoqoNV9fW2/BrwNLAG2ALsbpvtBq5sy1uA+6vqjap6DpgALklyHnB6VT1aVQXcM2PM9L72ApumzzokSYvnpMxZtMtD7wMOAOdW1UEYBApwTttsDfDi0LDJVlvTlmfWjxpTVUeAV4GzRnz9HUnGk4xPTU2djEOSJA2Zd1gkeTfwh8CvVNVfzbbpiFrNUp9tzNGFqjuqamNVbVy9enWvZUnSCZpXWCT5CQZB8dmq+lwrv9wuLdGeD7X6JLBuaPha4KVWXzuiftSYJCuBM4DD8+lZknTi5nM3VIA7gaer6reHVu0DtrXlbcCDQ/Wt7Q6n9Qwmsh9rl6peS3Jp2+c1M8ZM7+sq4JE2ryFJWkQr5zH254B/CTye5Jut9p+Bm4A9SbYDLwBXA1TVk0n2AE8xuJPquqp6s427FrgbOBV4qD1gEEb3JplgcEaxdR79SpLmaM5hUVX/h9FzCgCbjjFmF7BrRH0cuGhE/XVa2EiSlo7v4JYkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lS18qlbuB4JNkM/C6wAvhMVd20xC1pDsau/8JSt3Dcnr/pI0vdgrSsLPsziyQrgP8O/CJwAfCxJBcsbVeS9Pay7MMCuASYqKo/q6ofAPcDW5a4J0l6W3krXIZaA7w49HoS+MDwBkl2ADvay/+X5Jk5fq2zgb+Y49ilYL8LJDcDb6F+G/tdWH/X+/0Hs618K4RFRtTqqBdVdwB3zPsLJeNVtXG++1ks9ruw7Hdh2e/COtn9vhUuQ00C64ZerwVeWqJeJOlt6a0QFl8FNiRZn+QdwFZg3xL3JElvK8v+MlRVHUny74AvMrh19q6qenKBvty8L2UtMvtdWPa7sOx3YZ3UflNV/a0kSW9rb4XLUJKkJWZYSJK6DIsmyeYkzySZSHL9UvcDkGRdkj9O8nSSJ5N8otXPTLI/ybPtedXQmJ3tGJ5JcvkS9LwiyTeSfP4t0Ot7kuxN8p32Pf7gMu/3V9vPwRNJ7kvyzuXWb5K7khxK8sRQ7YR7THJxksfbuluTjLqFfqH6/c32M/HtJH+U5D3Lod9RvQ6t+7UkleTsBeu1qt72DwYT598F3gu8A/gWcMEy6Os84P1t+e8Df8rgI0/+G3B9q18P3NyWL2i9nwKsb8e0YpF7/o/A/wQ+314v5153A/+mLb8DeM9y7ZfBm1OfA05tr/cA/2q59Qv8PPB+4Imh2gn3CDwGfJDB+6weAn5xEfu9DFjZlm9eLv2O6rXV1zG4AejPgbMXqlfPLAaW5UeKVNXBqvp6W34NeJrBL40tDH7R0Z6vbMtbgPur6o2qeg6YYHBsiyLJWuAjwGeGysu119MZ/OO7E6CqflBVf7lc+21WAqcmWQmcxuD9Rsuq36r6MnB4RvmEekxyHnB6VT1ag99u9wyNWfB+q+pLVXWkvfwKg/d2LXm/x/jeAtwCfJKj36x80ns1LAZGfaTImiXqZaQkY8D7gAPAuVV1EAaBApzTNlvq4/gdBj+0fztUW669vheYAn6/XTb7TJJ3Ldd+q+p7wG8BLwAHgVer6kvLtd8ZTrTHNW15Zn0p/DKDv75hGfab5Arge1X1rRmrTnqvhsVA9yNFllKSdwN/CPxKVf3VbJuOqC3KcST5KHCoqr52vENG1Bbze76SwSn97VX1PuCvGVwiOZYl7bdd59/C4JLCTwHvSvLx2YaMqC2bn+nmWD0ui96T3AAcAT47XRqx2ZL1m+Q04AbgN0atHlGbV6+GxcCy/UiRJD/BICg+W1Wfa+WX2+kk7flQqy/lcfwccEWS5xlcxvvnSf5gmfY6/fUnq+pAe72XQXgs135/AXiuqqaq6ofA54CfXcb9DjvRHif50aWf4fqiSbIN+CjwS+1yDSy/fn+awR8P32r/7tYCX0/ykwvRq2ExsCw/UqTdpXAn8HRV/fbQqn3Atra8DXhwqL41ySlJ1gMbGExmLbiq2llVa6tqjMH375Gq+vhy7LX1+33gxSQ/00qbgKeWa78MLj9dmuS09nOxicEc1nLtd9gJ9dguVb2W5NJ2rNcMjVlwGfxna78OXFFVfzO0aln1W1WPV9U5VTXW/t1NMrgh5vsL0uvJnrF/qz6ADzO42+i7wA1L3U/r6Z8xOEX8NvDN9vgwcBbwMPBsez5zaMwN7RieYYHuIDmOvj/Ej+6GWra9Av8UGG/f3/8FrFrm/f5X4DvAE8C9DO50WVb9AvcxmFP5YfvltX0uPQIb23F+F/g92qdNLFK/Ewyu90//m/sfy6HfUb3OWP887W6ohejVj/uQJHV5GUqS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHX9fynbg8e6sbbWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(char_lens, bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "524abace-fd07-4b82-837a-c97984efd6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find what character length covers 95% of sequences\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6414d37e-19be-4348-bc63-388c4d920072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all keyboard characters for char-level embedding\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e9e995ad-bc61-482d-b783-6dd34085beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet)\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    name=\"char_vectorizer\"\n",
    "                                   )\n",
    "\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00f5b641-1d4b-4e34-ad78-aba1a1053911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters in character vocab: 28\n",
      "5 most common characters: ['', '[UNK]', 'e', 't', 'i']\n",
      "5 least common characters: ['k', 'x', 'z', 'q', 'j']\n"
     ]
    }
   ],
   "source": [
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91c801aa-40c3-409f-8548-f1d704739c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text:\n",
      "i n   t h e   c o n c u r r e n t   c h e m o r a d i o t h e r a p y   g r o u p   ,   t h e   o v e r a l l   r e s p o n s e   r a t e   w a s   @   %   (   @ / @   )   ,   t u m o r   c o n t r o l   r a t e   w a s   @   %   a n d   a   p a t h o l o g i c a l   c o m p l e t e   r e s p o n s e   w a s   a c h i e v e d   i n   @   %   (   @ / @   )   .\n",
      "\n",
      "Length of chars: 145\n",
      "\n",
      "Vectorized chars:\n",
      "[[ 4  6  3 13  2 11  7  6 11 16  8  8  2  6  3 11 13  2 15  7  8  5 10  4\n",
      "   7  3 13  2  8  5 14 19 18  8  7 16 14  3 13  2  7 21  2  8  5 12 12  8\n",
      "   2  9 14  7  6  9  2  8  5  3  2 20  5  9  3 16 15  7  8 11  7  6  3  8\n",
      "   7 12  8  5  3  2 20  5  9  5  6 10  5 14  5  3 13  7 12  7 18  4 11  5\n",
      "  12 11  7 15 14 12  2  3  2  8  2  9 14  7  6  9  2 20  5  9  5 11 13  4\n",
      "   2 21  2 10  4  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "\n",
      "Length of vectorized chars: 290\n"
     ]
    }
   ],
   "source": [
    "# Test out character vectorizer\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f\"Charified text:\\n{random_train_chars}\")\n",
    "print(f\"\\nLength of chars: {len(random_train_chars.split())}\")\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"\\nVectorized chars:\\n{vectorized_chars}\")\n",
    "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a02a28b3-a103-4a0a-a95e-11ff962c02a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text (before vectorization and embedding):\n",
      "i n   t h e   c o n c u r r e n t   c h e m o r a d i o t h e r a p y   g r o u p   ,   t h e   o v e r a l l   r e s p o n s e   r a t e   w a s   @   %   (   @ / @   )   ,   t u m o r   c o n t r o l   r a t e   w a s   @   %   a n d   a   p a t h o l o g i c a l   c o m p l e t e   r e s p o n s e   w a s   a c h i e v e d   i n   @   %   (   @ / @   )   .\n",
      "\n",
      "Embedded chars (after vectorization and embedding):\n",
      "[[[ 0.04262629  0.02349417 -0.04392989 ...  0.03827174 -0.02784822\n",
      "   -0.02209899]\n",
      "  [-0.02563715  0.03527446 -0.00010487 ...  0.02499387  0.00558852\n",
      "   -0.03972658]\n",
      "  [-0.01584969 -0.00284425 -0.00394547 ...  0.02918797  0.00757493\n",
      "    0.00877611]\n",
      "  ...\n",
      "  [ 0.0161504   0.00821089  0.00847595 ...  0.01383175 -0.032208\n",
      "    0.00835745]\n",
      "  [ 0.0161504   0.00821089  0.00847595 ...  0.01383175 -0.032208\n",
      "    0.00835745]\n",
      "  [ 0.0161504   0.00821089  0.00847595 ...  0.01383175 -0.032208\n",
      "    0.00835745]]]\n",
      "\n",
      "Character embedding shape: (1, 290, 25)\n"
     ]
    }
   ],
   "source": [
    "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS,\n",
    "                             output_dim=25,\n",
    "                              mask_zero=False,\n",
    "                              name=\"char_embed\"\n",
    "                             )\n",
    "\n",
    "\n",
    "# Test out character embedding layer\n",
    "print(f\"Charified text (before vectorization and embedding):\\n{random_train_chars}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b94dd4b8-100d-4a15-834c-3ba3fdbb2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Conv1D on chars only\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
    "char_vectors = char_vectorizer(inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "x = layers.Conv1D(64, kernel_size=5, padding=\"same\", activation=\"relu\")(char_embeddings)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(inputs=inputs,\n",
    "                         outputs=outputs,\n",
    "                         name=\"model_3_conv1D_char_embedding\")\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d19d4d7c-2c80-4c38-8df0-603f51b4ef79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_conv1D_char_embedding\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " char_vectorizer (TextVector  (None, 290)              0         \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " char_embed (Embedding)      (None, 290, 25)           1700      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 290, 64)           8064      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 64)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,089\n",
      "Trainable params: 10,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5bc04b25-e38d-4f7d-832f-3c1712d29453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create char datasets\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22e0d24e-804d-4aa7-8bb4-9f5b5c68955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 6s 6ms/step - loss: 1.2588 - accuracy: 0.4941 - val_loss: 1.0222 - val_accuracy: 0.5991\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 3s 6ms/step - loss: 0.9909 - accuracy: 0.6020 - val_loss: 0.9309 - val_accuracy: 0.6360\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 3s 6ms/step - loss: 0.9163 - accuracy: 0.6390 - val_loss: 0.8601 - val_accuracy: 0.6659\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on chars only\n",
    "model_3_history = model_3.fit(train_char_dataset,\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf363994-c8f3-4cfc-a5cc-a69682867003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 3s 4ms/step - loss: 0.8776 - accuracy: 0.6584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8776273131370544, 0.6583807468414307]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model_3 on whole validation char dataset\n",
    "model_3.evaluate(val_char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0eb87ea1-7695-44db-9a88-a7880963a6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 65.83807758506553,\n",
       " 'precision': 0.6505943799089137,\n",
       " 'recall': 0.6583807758506554,\n",
       " 'f1': 0.6471731050895714}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "# Calculate Conv1D char only model results\n",
    "model_3_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                        y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7341a73a-0801-45bf-87aa-2fb25fd00030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup token inputs/model\n",
    "token_inputs = layers.Input(shape=[], dtype=tf.string, name=\"token_input\")\n",
    "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
    "token_output = layers.Dense(128, activation=\"relu\")(token_embeddings)\n",
    "token_model = tf.keras.Model(inputs=token_inputs,\n",
    "                             outputs=token_output)\n",
    "\n",
    "# 2. Setup char inputs/model\n",
    "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name=\"char_input\")\n",
    "char_vectors = char_vectorizer(char_inputs)\n",
    "char_embeddings = char_embed(char_vectors)\n",
    "char_bi_lstm = layers.Bidirectional(layers.LSTM(25))(char_embeddings) # bi-LSTM shown in Figure 1 of https://arxiv.org/pdf/1612.05251.pdf\n",
    "char_model = tf.keras.Model(inputs=char_inputs,\n",
    "                            outputs=char_bi_lstm)\n",
    "\n",
    "# 3. Concatenate token and char inputs (create hybrid token embedding)\n",
    "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output, \n",
    "                                                                  char_model.output])\n",
    "\n",
    "# 4. Create output layers - addition of dropout discussed in 4.2 of https://arxiv.org/pdf/1612.05251.pdf\n",
    "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
    "combined_dense = layers.Dense(200, activation=\"relu\")(combined_dropout) # slightly different to Figure 1 due to different shapes of token/char embedding layers\n",
    "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
    "output_layer = layers.Dense(num_classes, activation=\"softmax\")(final_dropout)\n",
    "\n",
    "# 5. Construct model with char and token inputs\n",
    "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
    "                         outputs=output_layer,\n",
    "                         name=\"model_4_token_and_char_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "76890e21-6188-42ed-80cb-eea9b746f049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_token_and_char_embeddings\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " char_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " token_input (InputLayer)       [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " char_vectorizer (TextVectoriza  (None, 290)         0           ['char_input[0][0]']             \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " universal_sentence_encoder (Ke  (None, 512)         256797824   ['token_input[0][0]']            \n",
      " rasLayer)                                                                                        \n",
      "                                                                                                  \n",
      " char_embed (Embedding)         (None, 290, 25)      1700        ['char_vectorizer[2][0]']        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          65664       ['universal_sentence_encoder[2][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 50)          10200       ['char_embed[2][0]']             \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_char_hybrid (Concatenate  (None, 178)         0           ['dense_7[0][0]',                \n",
      " )                                                                'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 178)          0           ['token_char_hybrid[0][0]']      \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 200)          35800       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 200)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 5)            1005        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 256,912,193\n",
      "Trainable params: 114,369\n",
      "Non-trainable params: 256,797,824\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get summary of token and character model\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fe50a9c0-c23b-4bda-b995-ad4a11869f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile token char model\n",
    "model_4.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(), # section 4.2 of https://arxiv.org/pdf/1612.05251.pdf mentions using SGD but we'll stick with Adam\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "003b18b0-9c3a-4650-93f0-3387bd17e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine chars and tokens into a dataset\n",
    "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data\n",
    "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot) # make labels\n",
    "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels)) # combine data and labels\n",
    "\n",
    "# Prefetch and batch train data\n",
    "train_char_token_dataset = train_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE) \n",
    "\n",
    "# Repeat same steps validation data\n",
    "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
    "val_char_token_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
    "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_labels))\n",
    "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3dd3dda4-2add-4790-9f7c-81b715de7060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>,\n",
       " <PrefetchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out training char and token embedding dataset\n",
    "train_char_token_dataset, val_char_token_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c590eca4-c47e-4b05-b9b8-4a1baf38e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "562/562 [==============================] - 28s 46ms/step - loss: 0.9622 - accuracy: 0.6147 - val_loss: 0.7797 - val_accuracy: 0.6978\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 23s 41ms/step - loss: 0.7859 - accuracy: 0.6943 - val_loss: 0.7073 - val_accuracy: 0.7320\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 21s 38ms/step - loss: 0.7618 - accuracy: 0.7084 - val_loss: 0.6855 - val_accuracy: 0.7374\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on tokens and chars\n",
    "model_4_history = model_4.fit(train_char_token_dataset, # train on dataset of token and characters\n",
    "                              steps_per_epoch=int(0.1 * len(train_char_token_dataset)),\n",
    "                              epochs=3,\n",
    "                              validation_data=val_char_token_dataset,\n",
    "                              validation_steps=int(0.1 * len(val_char_token_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "aa95f202-63a6-4647-a3fb-4e86eb77f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 31s 33ms/step - loss: 0.6900 - accuracy: 0.7353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6899733543395996, 0.73533695936203]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on the whole validation dataset\n",
    "model_4.evaluate(val_char_token_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "60d50074-0be5-4371-b54d-89c0176476d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 73.53369522044221,\n",
       " 'precision': 0.7380634149433533,\n",
       " 'recall': 0.7353369522044221,\n",
       " 'f1': 0.733607453844797}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4_pred_probs = model_4.predict(val_char_token_dataset)\n",
    "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
    "model_4_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d432739-4fdc-4fc4-9635-9640e4968e08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
